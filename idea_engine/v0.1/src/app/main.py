#!/usr/bin/env python3
"""
ICLR RAG Service MVP - Dockerç‰ˆæœ¬
åŸºäºå·²éªŒè¯çš„MVPæœåŠ¡æ„å»ºçš„å®¹å™¨åŒ–Web APIæœåŠ¡
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import asyncio
import sys
import os
import time
import json
from typing import Optional, Dict, Any, List
import traceback

# å¯¼å…¥LightRAGç›¸å…³ç»„ä»¶
try:
    from lightrag import LightRAG, QueryParam
    from lightrag.utils import EmbeddingFunc
    from lightrag.kg.shared_storage import initialize_pipeline_status
    import aiohttp
    import numpy as np
except ImportError as e:
    print(f"å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿LightRAGç¯å¢ƒå·²æ­£ç¡®é…ç½®")

app = FastAPI(
    title="ICLR 2025 RAG Service MVP",
    description="åŸºäºLightRAGçš„å­¦æœ¯åˆ›æ–°å‘ç°APIæœåŠ¡",
    version="0.1.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹å®šä¹‰
class QueryRequest(BaseModel):
    query: str = Field(..., description="æŸ¥è¯¢æ–‡æœ¬", example="å›¾ç¥ç»ç½‘ç»œæœ‰å“ªäº›åˆ›æ–°ç‚¹ï¼Ÿ")
    mode: str = Field("hybrid", description="æŸ¥è¯¢æ¨¡å¼", example="hybrid", pattern="^(local|global|hybrid)$")
    enable_rerank: bool = Field(False, description="æ˜¯å¦å¯ç”¨é‡æ’")
    top_k: int = Field(10, description="è¿”å›ç»“æœæ•°é‡", ge=1, le=50)
    max_tokens: int = Field(4000, description="æœ€å¤§tokenæ•°", ge=100, le=8000)
    debug_mode: bool = Field(False, description="æ˜¯å¦è¿”å›æ£€ç´¢è¯¦ç»†ä¿¡æ¯ï¼ˆæµ‹è¯•ç”¨ï¼‰")

class RetrievalData(BaseModel):
    entities: List[Dict[str, Any]] = Field(default=[], description="æ£€ç´¢åˆ°çš„å®ä½“")
    relationships: List[Dict[str, Any]] = Field(default=[], description="æ£€ç´¢åˆ°çš„å…³ç³»")
    chunks: List[Dict[str, Any]] = Field(default=[], description="æ£€ç´¢åˆ°çš„æ–‡æœ¬å—")
    context: str = Field(default="", description="ä¸Šä¸‹æ–‡æ–‡æœ¬")

class QueryResponse(BaseModel):
    status: str = Field(..., description="è¯·æ±‚çŠ¶æ€")
    query: str = Field(..., description="åŸå§‹æŸ¥è¯¢")
    answer: str = Field(..., description="å›ç­”å†…å®¹")
    metadata: Dict[str, Any] = Field(..., description="å…ƒæ•°æ®ä¿¡æ¯")
    retrieval_data: Optional[RetrievalData] = Field(None, description="æ£€ç´¢è¯¦ç»†æ•°æ®ï¼ˆdebugæ¨¡å¼ï¼‰")

class HealthResponse(BaseModel):
    status: str
    service: str
    version: str
    rag_ready: bool
    timestamp: str

class StatsResponse(BaseModel):
    working_directory: str
    data_files: List[Dict[str, Any]]
    system_info: Dict[str, Any]

# ä»ç¯å¢ƒå˜é‡æˆ–æ–‡ä»¶è·å–APIé…ç½®
def get_api_key():
    # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è·å–
    api_key = os.getenv("LLM_API_KEY")
    if api_key and api_key != "your_api_key_here":
        return api_key

    # ä»æ–‡ä»¶è·å–ï¼ˆæ”¯æŒå®¹å™¨å†…å’Œæœ¬åœ°ï¼‰
    key_files = [
        "/app/secrets/llm_api_key.txt",  # å®¹å™¨å†…è·¯å¾„
        "secrets/llm_api_key.txt",      # æœ¬åœ°å¼€å‘è·¯å¾„
        "../secrets/llm_api_key.txt"    # å¤‡ç”¨è·¯å¾„
    ]

    for key_file in key_files:
        try:
            if os.path.exists(key_file):
                with open(key_file, 'r', encoding='utf-8') as f:
                    key = f.read().strip()
                    if key and not key.startswith("your_"):
                        return key
        except Exception:
            continue

    return "your_api_key_here"

GPT_CONFIG = {
    "api_key": get_api_key(),
    "base_url": os.getenv("LLM_BASE_URL", "https://proxy.infix-ai.xyz"),
    "model": os.getenv("LLM_MODEL", "gpt-4o-mini")
}

# å…¨å±€RAGå®ä¾‹
rag_engine = None
service_stats = {
    "queries_count": 0,
    "total_response_time": 0.0,
    "errors_count": 0,
    "start_time": time.time()
}

async def gpt_llm_func(prompt, system_prompt=None, history_messages=[], **kwargs) -> str:
    """GPT-4o-mini LLMè°ƒç”¨å‡½æ•°"""

    headers = {
        "Authorization": f"Bearer {GPT_CONFIG['api_key']}",
        "Content-Type": "application/json"
    }

    messages = []

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    # æ·»åŠ å†å²æ¶ˆæ¯
    for msg in history_messages:
        messages.append(msg)

    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": prompt})

    payload = {
        "model": GPT_CONFIG["model"],
        "messages": messages,
        "max_tokens": kwargs.get("max_tokens", 4000),
        "temperature": kwargs.get("temperature", 0.0)
    }

    async with aiohttp.ClientSession() as session:
        url = f"{GPT_CONFIG['base_url']}/v1/chat/completions"

        async with session.post(url, json=payload, headers=headers) as response:
            if response.status == 200:
                result = await response.json()
                return result["choices"][0]["message"]["content"]
            else:
                error = await response.text()
                raise Exception(f"GPT-4o-mini APIå¤±è´¥: {response.status}, {error}")

async def openai_embedding_func(texts: list[str]) -> np.ndarray:
    """OpenAI embeddingå‡½æ•°"""

    headers = {
        "Authorization": f"Bearer {GPT_CONFIG['api_key']}",
        "Content-Type": "application/json"
    }

    payload = {
        "input": texts,
        "model": "text-embedding-3-small"
    }

    async with aiohttp.ClientSession() as session:
        url = f"{GPT_CONFIG['base_url']}/v1/embeddings"

        async with session.post(url, json=payload, headers=headers) as response:
            if response.status == 200:
                result = await response.json()
                embeddings = []
                for item in result.get("data", []):
                    embeddings.append(item.get("embedding", []))

                if embeddings:
                    return np.array(embeddings, dtype=np.float32)

            raise Exception(f"Embeddingå¤±è´¥: {response.status}")

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç³»ç»Ÿ"""
    global rag_engine

    try:
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ICLR RAG MVPæœåŠ¡...")

        # å®¹å™¨å†…æ•°æ®ç›®å½•
        working_dir = "/app/data"
        print(f"ğŸ“‚ æ•°æ®ç›®å½•: {working_dir}")

        if not os.path.exists(working_dir):
            raise Exception(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {working_dir}")

        rag_engine = LightRAG(
            working_dir=working_dir,
            llm_model_func=gpt_llm_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                func=openai_embedding_func
            )
        )

        await rag_engine.initialize_storages()
        await initialize_pipeline_status()

        print("âœ… ICLR RAG MVPæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    except Exception as e:
        print(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        # ä¸ä¸­æ–­å¯åŠ¨ï¼Œä½†æ ‡è®°ä¸ºæœªå°±ç»ª
        rag_engine = None

@app.post("/api/v1/query", response_model=QueryResponse, summary="æ‰§è¡Œå­¦æœ¯æŸ¥è¯¢", description="æŸ¥è¯¢ICLR 2025è®ºæ–‡æ•°æ®åº“ï¼Œæ”¯æŒåˆ›æ–°ç‚¹å‘ç°ã€ç§‘ç ”æ€è·¯ç­‰å¤šç§æŸ¥è¯¢ç±»å‹")
async def query_papers(request: QueryRequest):
    """æ ¸å¿ƒæŸ¥è¯¢æ¥å£ - æ”¯æŒå¤šç§å­¦æœ¯æŸ¥è¯¢åœºæ™¯"""
    global service_stats

    try:
        if rag_engine is None:
            service_stats["errors_count"] += 1
            raise HTTPException(status_code=503, detail="RAGå¼•æ“æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥æœåŠ¡åˆå§‹åŒ–çŠ¶æ€")

        # è®°å½•æŸ¥è¯¢å¼€å§‹
        start_time = time.time()
        service_stats["queries_count"] += 1

        # æ„å»ºæŸ¥è¯¢å‚æ•°
        query_param = QueryParam(
            mode=request.mode,
            enable_rerank=request.enable_rerank,
            top_k=request.top_k
        )

        print(f"ğŸ” æ‰§è¡ŒæŸ¥è¯¢: {request.query[:50]}...")

        # æ ¹æ®debug_modeå†³å®šæ˜¯å¦è·å–è¯¦ç»†æ£€ç´¢æ•°æ®
        retrieval_data = None
        if request.debug_mode:
            # è·å–æ£€ç´¢è¯¦ç»†æ•°æ®
            raw_data = await rag_engine.aquery_data(
                request.query,
                param=query_param
            )

            # æ£€æŸ¥ raw_data çš„ç±»å‹å¹¶æ­£ç¡®å¤„ç†
            if isinstance(raw_data, dict):
                retrieval_data = RetrievalData(
                    entities=raw_data.get("entities", []),
                    relationships=raw_data.get("relationships", []),
                    chunks=raw_data.get("chunks", []),
                    context=raw_data.get("context", "")
                )
            else:
                # å¦‚æœ aquery_data è¿”å›å­—ç¬¦ä¸²ï¼Œåˆ™åˆ›å»ºç©ºçš„æ£€ç´¢æ•°æ®
                print(f"âš ï¸ aquery_data è¿”å›äº†éå­—å…¸ç±»å‹: {type(raw_data)}")
                retrieval_data = RetrievalData(
                    entities=[],
                    relationships=[],
                    chunks=[],
                    context=str(raw_data) if raw_data else ""
                )

            # åŒæ—¶è·å–LLMå›ç­”
            result = await rag_engine.aquery(
                request.query,
                param=query_param
            )
        else:
            # åªè·å–LLMå›ç­”
            result = await rag_engine.aquery(
                request.query,
                param=query_param
            )

        end_time = time.time()
        response_time = end_time - start_time
        service_stats["total_response_time"] += response_time

        print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œè€—æ—¶: {response_time:.2f}ç§’")

        return QueryResponse(
            status="success",
            query=request.query,
            answer=result,
            metadata={
                "response_time": round(response_time, 2),
                "answer_length": len(result),
                "mode": request.mode,
                "rerank_enabled": request.enable_rerank,
                "top_k": request.top_k,
                "debug_mode": request.debug_mode,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            },
            retrieval_data=retrieval_data
        )

    except Exception as e:
        service_stats["errors_count"] += 1
        error_msg = f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        raise HTTPException(status_code=500, detail=error_msg)

@app.get("/api/v1/health", response_model=HealthResponse, summary="å¥åº·æ£€æŸ¥", description="æ£€æŸ¥æœåŠ¡è¿è¡ŒçŠ¶æ€å’ŒRAGå¼•æ“å°±ç»ªçŠ¶æ€")
async def health_check():
    """æœåŠ¡å¥åº·æ£€æŸ¥"""
    return HealthResponse(
        status="healthy" if rag_engine is not None else "degraded",
        service="ICLR RAG Service MVP",
        version="0.1.0",
        rag_ready=rag_engine is not None,
        timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
    )

@app.get("/api/v1/stats", response_model=StatsResponse, summary="ç³»ç»Ÿç»Ÿè®¡", description="è·å–æ•°æ®æ–‡ä»¶çŠ¶æ€å’ŒæœåŠ¡è¿è¡Œç»Ÿè®¡")
async def get_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    working_dir = "/app/data"

    stats = {
        "working_directory": working_dir,
        "data_files": [],
        "system_info": {
            "queries_count": service_stats["queries_count"],
            "errors_count": service_stats["errors_count"],
            "uptime_seconds": round(time.time() - service_stats["start_time"], 2),
            "avg_response_time": round(
                service_stats["total_response_time"] / max(service_stats["queries_count"], 1), 2
            ),
            "success_rate": round(
                (service_stats["queries_count"] - service_stats["errors_count"]) / max(service_stats["queries_count"], 1) * 100, 2
            ) if service_stats["queries_count"] > 0 else 100.0
        }
    }

    # æ£€æŸ¥å…³é”®æ•°æ®æ–‡ä»¶
    data_files = [
        "graph_chunk_entity_relation.graphml",
        "vdb_entities.json",
        "vdb_relationships.json",
        "vdb_chunks.json",
        "kv_store_full_docs.json"
    ]

    for filename in data_files:
        filepath = os.path.join(working_dir, filename)
        if os.path.exists(filepath):
            size = os.path.getsize(filepath)
            stats["data_files"].append({
                "filename": filename,
                "size_bytes": size,
                "size_mb": round(size / 1024 / 1024, 2),
                "exists": True
            })
        else:
            stats["data_files"].append({
                "filename": filename,
                "exists": False,
                "size_bytes": 0,
                "size_mb": 0
            })

    return StatsResponse(**stats)

@app.get("/api/v1/query/examples", summary="æŸ¥è¯¢ç¤ºä¾‹", description="è·å–å¸¸è§æŸ¥è¯¢ç¤ºä¾‹")
async def get_query_examples():
    """æä¾›æŸ¥è¯¢ç¤ºä¾‹"""
    examples = {
        "åˆ›æ–°ç‚¹æŸ¥è¯¢": [
            "å›¾ç¥ç»ç½‘ç»œæœ‰å“ªäº›æœ€æ–°åˆ›æ–°ç‚¹ï¼Ÿ",
            "è‡ªç›‘ç£å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„åˆ›æ–°çªç ´",
            "Transformeræ¶æ„çš„æœ€æ–°æ”¹è¿›æœ‰å“ªäº›ï¼Ÿ",
            "å¼ºåŒ–å­¦ä¹ åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åˆ›æ–°åº”ç”¨"
        ],
        "ç§‘ç ”æ€è·¯æŸ¥è¯¢": [
            "å¦‚ä½•æ”¹è¿›ç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œæ–¹æ³•ï¼Ÿ",
            "è§£å†³é•¿åºåˆ—å»ºæ¨¡é—®é¢˜çš„ç ”ç©¶æ–¹å‘",
            "æå‡æ¨¡å‹å¯è§£é‡Šæ€§çš„æŠ€æœ¯è·¯çº¿",
            "é™ä½å¤§æ¨¡å‹è®¡ç®—æˆæœ¬çš„æ–¹æ³•æ¢ç´¢"
        ],
        "æŠ€æœ¯å¯¹æ¯”æŸ¥è¯¢": [
            "ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ£åŠ¿å¯¹æ¯”",
            "ç›‘ç£å­¦ä¹ ä¸è‡ªç›‘ç£å­¦ä¹ çš„æ•ˆæœæ¯”è¾ƒ",
            "CNNã€RNNå’ŒTransformeråœ¨NLPä»»åŠ¡ä¸­çš„è¡¨ç°",
            "ä¸åŒä¼˜åŒ–ç®—æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„é€‚ç”¨åœºæ™¯"
        ],
        "åº”ç”¨åœºæ™¯æŸ¥è¯¢": [
            "å›¾ç¥ç»ç½‘ç»œåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨",
            "å¤§è¯­è¨€æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆä¸­çš„ä½¿ç”¨",
            "è®¡ç®—æœºè§†è§‰æŠ€æœ¯åœ¨åŒ»ç–—è¯Šæ–­ä¸­çš„åº”ç”¨",
            "å¼ºåŒ–å­¦ä¹ åœ¨è‡ªåŠ¨é©¾é©¶ä¸­çš„å®è·µ"
        ],
        "è¶‹åŠ¿åˆ†ææŸ¥è¯¢": [
            "å½“å‰æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç ”ç©¶çƒ­ç‚¹",
            "æœªæ¥AIå‘å±•çš„ä¸»è¦æ–¹å‘",
            "æ–°å…´æŠ€æœ¯åœ¨å­¦æœ¯ç•Œçš„æ¥å—åº¦",
            "è·¨å­¦ç§‘ç ”ç©¶çš„å‘å±•è¶‹åŠ¿"
        ]
    }

    return {
        "status": "success",
        "examples": examples,
        "usage_tips": [
            "ä½¿ç”¨å…·ä½“çš„æŠ€æœ¯æœ¯è¯­å¯ä»¥è·å¾—æ›´ç²¾ç¡®çš„ç»“æœ",
            "é—®é¢˜å¯ä»¥åŒ…å«å¤šä¸ªæ¦‚å¿µï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å…³è”ç›¸å…³è®ºæ–‡",
            "æ”¯æŒä¸­è‹±æ–‡æŸ¥è¯¢ï¼Œå»ºè®®ä½¿ç”¨ä¸­æ–‡è·å¾—æ›´å¥½çš„ç†è§£",
            "å¯ä»¥é€šè¿‡modeå‚æ•°è°ƒæ•´æ£€ç´¢ç­–ç•¥ï¼šlocal(å±€éƒ¨)ã€global(å…¨å±€)ã€hybrid(æ··åˆ)",
            "è®¾ç½®debug_mode=trueå¯ä»¥è·å–æ£€ç´¢çš„å®ä½“ã€å…³ç³»å’Œæ–‡æœ¬å—è¯¦ç»†ä¿¡æ¯ï¼ˆæµ‹è¯•åŠŸèƒ½ï¼‰"
        ]
    }

@app.get("/", summary="æœåŠ¡æ ¹é¡µé¢")
async def root():
    """æœåŠ¡æ ¹é¡µé¢"""
    return {
        "service": "ICLR 2025 RAG Service MVP",
        "version": "0.1.0",
        "description": "åŸºäºLightRAGçš„å­¦æœ¯åˆ›æ–°å‘ç°APIæœåŠ¡",
        "docs": "/docs",
        "health": "/api/v1/health",
        "examples": "/api/v1/query/examples"
    }

if __name__ == "__main__":
    import uvicorn

    print("ğŸŒŸ å¯åŠ¨ICLR RAG MVPæœåŠ¡...")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8002/docs")
    print("ğŸ” å¥åº·æ£€æŸ¥: http://localhost:8002/api/v1/health")
    print("ğŸ’¡ æŸ¥è¯¢ç¤ºä¾‹: http://localhost:8002/api/v1/query/examples")

    uvicorn.run(app, host="0.0.0.0", port=8002)